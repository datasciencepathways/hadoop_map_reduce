{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hadoop_install_on_colab.ipynb","provenance":[{"file_id":"https://github.com/apanqasem/hadoop_tutorials/blob/main/hadoop_install_on_colab.ipynb","timestamp":1658784679989}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"myPIGP-mwKBD"},"source":["##**Hadoop On Google Colab**\n","**Module 2.2**  \n","**Block 9: Big Data Processing and NLP**\n","\n","<a href=\"https://colab.research.google.com/github/datasciencepathways/hadoop_map_reduce/blob/main/tutorials/hadoop_install_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"," \n","In our previous tutorial we went through the steps of installing a single-node, pseude-distributed Hadoop cluster on a Linux system. This tutorial will describes how to build and configure a single-node Hadoop system on [Google Colab](https://colab.research.google.com/). Google Colab is a free Jupyter notebook environment that runs in the cloud. \n","As we will setting up Hadoop is much easier then setting it up on your system. However, the downside is that you will repeat the installation step each time you launch a Google Colab. The reason for this is that Google Colab gives a new VM each time you open a notebook. Nontheless, Google Colab is a great sandbox for getting some practice with Hadoop and distributed processing, among many other things.  \n","\n","The tutorial should not take more than more 20 minutes. You do not have to complete it in one sitting, however. \n","\n","The tutorial has been written in a way such that all of the commands work out of the box in Google Colab. However, if a particular command does not work you get a weird error message, please add your question to the discussion forum.\n","\n","The main steps for setting up Hadoop on Google Colab are listed below.\n","\n","[Getting Ready](#getting_ready)  \n","[Download and Installation](#download)  \n","[Configuration](#config)   \n","[Running and Testing](#running)  \n","[Conclusion](#end)  \n"]},{"cell_type":"markdown","metadata":{"id":"j9bT9M1yvyXG"},"source":["## <a name=\"getting_ready\"></a> Getting Ready \n","The Google Colab environment should have all the necessary software and packages to run Hadoop. \n","\n","But just as a precaution make sure that a recent version of Jave Runtime Environment (JRE) is installed. \n"]},{"cell_type":"code","source":[" !java -version "],"metadata":{"id":"j3n50cDCGzx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <a name=\"download\"></a>Download and Installation \n","\n","You will need to download Hadoop from the [Apache Mirror server](https://dlcdn.apache.org/hadoop/common/). To download version 3.3.1, use the following command \n"],"metadata":{"id":"1zCjqOJeHIBe"}},{"cell_type":"code","metadata":{"id":"bijZAdD_cBMK"},"source":["!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now untar and unzip the archive using the `tar` command. "],"metadata":{"id":"Tgp2ime1HZnt"}},{"cell_type":"code","metadata":{"id":"nVce513-cBHm"},"source":["!tar -xzvf hadoop-3.3.1.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The command should take a few seconds to run and you should see lots of output on the screend. If the untar command completed successfuly, the Hadoop binaries and libraries should now be in the `hadoop-3.3.1` directory. Check to see if the binaries are indeed there. \n"],"metadata":{"id":"XPTJWWRKHrEq"}},{"cell_type":"code","source":["!ls hadoop-3.3.1/bin"],"metadata":{"id":"83DbRdnJHpR0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Right now, the installation of Hadoop is in the local directory. Although it does not really matter in Colab, it's generally a good idea to move it to a system location. On Colab, you always have `root` access on the VM that is given to you. So you can use the `cp` command to copy Hadoop to `/usr/local`"],"metadata":{"id":"6ahgyAezIhbG"}},{"cell_type":"code","metadata":{"id":"JF-ze-YOdync"},"source":["!cp -r hadoop-3.3.1/ /usr/local/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <a name=\"config\"></a>Configuration \n","\n","Hadoop is now installed on your system. But before we can run it we need to modify on configuration file."],"metadata":{"id":"Li_dfrXrKl_u"}},{"cell_type":"markdown","metadata":{"id":"Vh6Dqbbrwqpe"},"source":["Hadoop requires that you set the path to Java, either as an environment variable or in the Hadoop configuration file. Just like Linux systems, Google Colab uses a symbolic link to the Java installation. To get the _actual_ location of Java use the following command. "]},{"cell_type":"code","metadata":{"id":"_OUc19ZtcBG5"},"source":["!readlink -f /usr/bin/java "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, use the folder navigation pane on the left to browse to the file `usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh`. Double-click on the file to open it for editing. Uncomment the line begins with `export JAVA_HOME=` (should be line 54). Then add the Java path after the `=`\n","\n","```bash\n","export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\n","```"],"metadata":{"id":"OEwDUJaWcVKc"}},{"cell_type":"markdown","source":["### <a name=\"format\"></a>Formatting the `namenode` \n","\n","Now, we have Hadoop installed and configured. There is one more step before we can run the Hadoop cluster. The `namenode` must be fortmatted with `hdfs`. This is not the same as formatting or re-formatting a hard disk. This just ensure that the `namenode` has all the information to work with the target hard disk.  \n"],"metadata":{"id":"RBgh3ue5Nvlf"}},{"cell_type":"code","source":["!/usr/local/hadoop-3.3.1/bin/hdfs namenode -format"],"metadata":{"id":"FkKMT--cN01v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oj00rPPZyEWZ"},"source":["## <a name=\"running\"></a>Running and Testing \n","\n","At this stage, we should be all set to run the Hadoop single-node cluster. To start the cluster, we need to invoke a script that is the in Hadoop sbin directory. "]},{"cell_type":"code","metadata":{"id":"Zhf-zK7NcBDF"},"source":["!/usr/local/hadoop-3.3.1/bin/hadoop"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <a name=\"end\"></a>Conclusion\n","\n","That's it! You now have a working Hadoop system in Google Colab."],"metadata":{"id":"LZIPAfyAO5Ap"}}]}