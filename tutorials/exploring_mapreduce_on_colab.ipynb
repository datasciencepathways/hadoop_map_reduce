{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/datasciencepathways/hadoop_map_reduce/blob/main/tutorials/exploring_mapreduce_on_colab.ipynb","timestamp":1685119490105},{"file_id":"https://github.com/apanqasem/hadoop_tutorials/blob/main/exploring_mapreduce_on_colab.ipynb","timestamp":1658785371314}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"myPIGP-mwKBD"},"source":["##**Exploring MapReduce Options**\n","**Module 2, Section 3.2**  \n","**Block 9: Big Data Processing and NLP**\n","\n","<a href=\"https://colab.research.google.com/github/datasciencepathways/hadoop_map_reduce/blob/main/tutorials/exploring_mapreduce_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"," \n","In our previous tutorial we went through the steps of running a simple MapReduce job on Google Colab. This tutorial will look at some of the parameters that allow you to control the exeuction of MapReduce jobs. \n","\n","The tutorial should not take more than 30 minutes to complete. \n","\n","The tutorial has been written in a way such that all commands work out of the box in Google Colab. However, if a particular command does not work or you get a weird error message, please add your question to the discussion forum.\n","\n","The main sections of this tutorial are listed below. \n","\n","\n","1. [Hadoop Install](#hadoop)\n","2. [Adding Functionality to Mapper Code](#code)\n","3. [Working with Real Datasets](#datasets) \n","4. [Controlling Hadoop Job Parameters](#runtime)\n","5. [Analyzing the Output](#output)\n","6. [Measuring Performance](#performance)\n","7. [Conclusion](#end)\n"]},{"cell_type":"markdown","metadata":{"id":"j9bT9M1yvyXG"},"source":["## <a name=\"hadoop\"></a>Hadoop Install\n","Since the Google Colab environment is refreshed each time you open a new notebook, we will first need to install Hadoop in this VM instance. You can just follow the steps from the previous tutorial. For convenience, the sequence of commands for Hadoop installation is given below. "]},{"cell_type":"code","metadata":{"id":"bijZAdD_cBMK"},"source":["!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nVce513-cBHm"},"source":["!tar -xzvf hadoop-3.3.1.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls hadoop-3.3.1/bin"],"metadata":{"id":"83DbRdnJHpR0"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JF-ze-YOdync","executionInfo":{"status":"ok","timestamp":1685119848887,"user_tz":300,"elapsed":11474,"user":{"displayName":"Apan Qasem","userId":"08611280596483031752"}}},"source":["!cp -r hadoop-3.3.1/ /usr/local/"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OUc19ZtcBG5"},"source":["!readlink -f /usr/bin/java | sed 's/bin\\/java//'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, use the folder navigation pane on the left to browse to the file `/usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh`. Double-click on the file to open it for editing. Uncomment the line begins with `export JAVA_HOME=` (should be line 54). Then add the Java path after the `=`\n","\n","```bash\n","export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\n","```"],"metadata":{"id":"OEwDUJaWcVKc"}},{"cell_type":"code","source":["!/usr/local/hadoop-3.3.1/bin/hdfs namenode -format"],"metadata":{"id":"FkKMT--cN01v"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zhf-zK7NcBDF"},"source":["!/usr/local/hadoop-3.3.1/bin/hadoop"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##<a name=\"code\"></a>Adding Functionality to Mapper Code \n","\n","In this tutorial, we will again be working with the Word Count MapReduce program. \n","\n","Obtain the MapReduce code from the Block 9 Git repo. "],"metadata":{"id":"MMh5I6cWSrDv"}},{"cell_type":"code","source":["!git clone https://github.com/datasciencepathways/hadoop_map_reduce.git"],"metadata":{"id":"DT9C-5zVT5aq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this tutorial, we will utilize a slightly more advanced version of the mapper. The `hadoop_map_reduce/mapper_advanced.py` will still count all the words but skip some of the more common words like articles (a, an, the) and some common prepositions and verbs. This is often a useful strategy for dealing with large text datasets. Counting the common words can skew the final analysis and also make the program run for an unnecessarily longer time. You can take a look at the two versions of the mapper codes and see how we eliminate the common words from consideration. \n","\n","There is no change to the reducer code. \n","\n","As before, for convenience we will copy the mapper and reducer to the current directory. "],"metadata":{"id":"_jGVp6TuUHVS"}},{"cell_type":"code","source":["!cp hadoop_map_reduce/code/mapper_advanced.py ."],"metadata":{"id":"iNbrL7Lmgb8b","executionInfo":{"status":"ok","timestamp":1685120438022,"user_tz":300,"elapsed":225,"user":{"displayName":"Apan Qasem","userId":"08611280596483031752"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!cp hadoop_map_reduce/code/reducer.py ."],"metadata":{"id":"v7WDIwQ5gm1i","executionInfo":{"status":"ok","timestamp":1685120440902,"user_tz":300,"elapsed":181,"user":{"displayName":"Apan Qasem","userId":"08611280596483031752"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["To be able to execute these codes, we will need to set the execute permission on the two files."],"metadata":{"id":"ZyvRcSuFV0Bm"}},{"cell_type":"code","source":["!chmod u+x mapper_advanced.py\n","!chmod u+rwx reducer.py"],"metadata":{"id":"RggUUHNfgsIR","executionInfo":{"status":"ok","timestamp":1685120457078,"user_tz":300,"elapsed":363,"user":{"displayName":"Apan Qasem","userId":"08611280596483031752"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["##<a name=\"datasets\"></a>Working with Real Datasets\n","\n","For any Hadoop job, we need to provide the names of an `input` and an `output` directory. The input directory is the place where the program is going to look for its input data. The output directory is the location of where the output is going to be written. \n","\n","These directories can be given any names. By convention, the names typically contain input/output as a suffix or a prefix. Let's create the input directory. "],"metadata":{"id":"LZIPAfyAO5Ap"}},{"cell_type":"code","metadata":{"id":"uI-YBPIzcBCA","executionInfo":{"status":"ok","timestamp":1685120500428,"user_tz":300,"elapsed":238,"user":{"displayName":"Apan Qasem","userId":"08611280596483031752"}}},"source":["!mkdir test_input"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["The output directory will be automatically created when the Hadoop job runs. "],"metadata":{"id":"R2xVEFgWSA9X"}},{"cell_type":"markdown","source":["In the previous tutorial, we created our own toy datasets to test our Hadoop program. In general though, you will be working with real datasets that are much larger. \n","\n","There are many free datasets that are available in the public domain. These are excellent candidates for evaluating your Hadoop MapReduce algorithms. Below are two useful sources\n","\n","* [Kaggle Datasets](https://www.kaggle.com/datasets)  \n","* [Open-source Datasets for Text Classificatoin](https://analyticsindiamag.com/10-open-source-datasets-for-text-classification/)\n","\n","Some of theses datasets are large and can take a long time to download. For your convenience we have copeid portions of these datasets into our course Git repository. When you cloned the repo, the datasets were downloaded with it. \n","\n","In this tutorial we will be working with the Lord of the Rings dataset which contains the full text of the [LOTR trilogy](https://en.wikipedia.org/wiki/The_Lord_of_the_Rings). \n","\n","Let's copy the files into our input directory. "],"metadata":{"id":"E6IlYraCSlmh"}},{"cell_type":"code","source":["!cp hadoop_map_reduce/code/datasets/LOTR/*.txt test_input/"],"metadata":{"id":"GvBOgTvvdUUq","executionInfo":{"status":"ok","timestamp":1685120607341,"user_tz":300,"elapsed":175,"user":{"displayName":"Apan Qasem","userId":"08611280596483031752"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["##<a name=\"runtime\"></a>Controlling Hadoop Job Parameters \n","\n","Now we are all set to run our MapReduce Hadoop job. \n","\n","As a reminder, the general format for the command for running a MapReduce Hadoop job is as follows: \n","\n","```bash \n","hadoop jar hadoop-streaming.jar \\\n","-input name_of_input_file \\\n","-output name_of_output_directory \\\n","-file name_of_mapper_file \\\n","-mapper the_mapper_cmd \\\n","-file name_of_reducer_file \\\n","-reducer the_reducer_cmd \\\n","```\n","\n","### Selecting input files\n","We will first run our Word Count MapReduce on the Fellowship of the Ring text. Note, we do not need to type the full name of the input file. We can use a wildcard to let the system figure out which files we want to use for this MapReduce job. When we say, `the_fellowship*.txt`, the system will look for all files in the input directory that begin with the word `the_fellowship`.  "],"metadata":{"id":"lItkk4TLW24E"}},{"cell_type":"code","source":["!rm -rf test_output\n","!/usr/local/hadoop-3.3.1/bin/hadoop jar /usr/local/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar -input test_input/the_fellowship*.txt -output test_output -file mapper_advanced.py  -file reducer.py  -mapper 'python mapper_advanced.py'  -reducer 'python reducer.py'"],"metadata":{"id":"aYShAt24g-P-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If everything went ok, you should see a whole bunch of output. The last line should give you the name of the output directory. \n","\n","Let's check the contents of the output directory. "],"metadata":{"id":"Wzt7xscjVDsI"}},{"cell_type":"code","metadata":{"id":"mtr0xWbfcA5J"},"source":["!ls -ltr test_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`part-00000` is the file that should contain the results of output of the word count program. You should see a recent timestamp on this file. Let's check the contents of that file. Because there are _many_ words in LOTR, we will just look at the count for the first 200 words using the Linux `head` command (`| head -200`)"],"metadata":{"id":"4RMjutnYd0dF"}},{"cell_type":"code","source":["!cat test_output/part-00000 | head -200"],"metadata":{"id":"WrAS6cQTfClZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##<a name=\"output\"></a>Analyzing the Output \n","\n","Understandabaly, the output file is quite large and the data by itself does not give us any insight about the writing style of Tolkien. Typically, we would feed the word count data to another algorithm to analyze. Here, we will simply sort the output by occurrennce count to find out which words appear most frequently in The Fellowship of the Ring. \n","\n","We can use the Linux `sort` utility to sort the ouput. We will sort the words in descending order (`-r` flag) and just look at the top 100 (`| head -100`) "],"metadata":{"id":"YnCkHIkz036I"}},{"cell_type":"code","source":["!sort -n -k 2 -r test_output/part-00000 | head -100"],"metadata":{"id":"e3gN2WgK2ESq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Does the output make sense? "],"metadata":{"id":"z6t_JRdv2QbD"}},{"cell_type":"markdown","source":["##<a name=\"performance\"></a>Measuring Performance\n","\n","Often times we are interested in how well the code performs. We can time the execution of the MapReduce code using the Linux `time` utility. To do this we will simply place the time command before the Hadoop MapReduce run command. This will re-run the MapReduce job and measure how long it took. "],"metadata":{"id":"qDsutXER2YAD"}},{"cell_type":"code","source":["!rm -rf test_output\n","!time /usr/local/hadoop-3.3.1/bin/hadoop jar /usr/local/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar -input test_input/the_*.txt -output test_output -file mapper_advanced.py  -file reducer.py  -mapper 'python mapper_advanced.py'  -reducer 'python reducer.py'"],"metadata":{"id":"beJCru1H2sGb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `time` commands gives us three times. The `real` time is total time elapsed and primarily what we are interested in. The `user` time, which is typically greater than `real` time is a measure of parallelism. The `sys` time is the time spent doing other work in the system. For more on the time command look at this [tutorial](https://www.journaldev.com/43534/time-command-in-linux)"],"metadata":{"id":"2sq9C2a83Pq-"}},{"cell_type":"markdown","source":["## <a name=\"end\"></a>Conclusion\n","\n","That's it, you are done with this tutorial. "],"metadata":{"id":"LCYWXrHfelrd"}}]}