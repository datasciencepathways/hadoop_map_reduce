{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myPIGP-mwKBD"
   },
   "source": [
    "## **Running Hadoop MapReduce On Google Colab**\n",
    "**Module 2, Section 3.1**  \n",
    "**Block 9: Big Data Processing and NLP**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/datasciencepathways/hadoop_map_reduce/blob/main/tutorials/running_mapreduce_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    " \n",
    "In our previous tutorial we went through the steps of installing a single-node, pseudo-distributed Hadoop cluster on Google Colab. This tutorial will show you how to run a simple MapReduce job on Hadoop. \n",
    "\n",
    "The tutorial should not take more than 30 minutes to complete. \n",
    "\n",
    "The tutorial has been written in a way such that all commands work out of the box in Google Colab. However, if a particular command does not work or you get a weird error message, please add your question to the discussion forum.\n",
    "\n",
    "The main steps for running a MapReduce Hadoop job on Google Colab are listed below.\n",
    "\n",
    "1. [Hadoop Install](#hadoop)\n",
    "2. [Running WordCount](#wordcount)\n",
    "3. [Conclusion](#end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9bT9M1yvyXG"
   },
   "source": [
    "## <a name=\"hadoop\"></a>Hadoop Install\n",
    "Since the Google Colab environment is refreshed each time you open a new notebook, we will first need to install Hadoop in this VM instance. You can just follow the steps for the previous tutorial. For convenience, the sequence of commands for Hadoop installation is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bijZAdD_cBMK"
   },
   "outputs": [],
   "source": [
    "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVce513-cBHm"
   },
   "outputs": [],
   "source": [
    "!tar -xzvf hadoop-3.3.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83DbRdnJHpR0"
   },
   "outputs": [],
   "source": [
    "!ls hadoop-3.3.1/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JF-ze-YOdync"
   },
   "outputs": [],
   "source": [
    "!cp -r hadoop-3.3.1/ /usr/local/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OUc19ZtcBG5"
   },
   "outputs": [],
   "source": [
    "!readlink -f /usr/bin/java | sed 's/bin\\/java//'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEwDUJaWcVKc"
   },
   "source": [
    "Now, use the folder navigation pane on the left to browse to the file `/usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh`. Double-click on the file to open it for editing. Uncomment the line that begins with `export JAVA_HOME=` (should be line 54). Then add the Java path after the `=`\n",
    "\n",
    "```bash\n",
    "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkKMT--cN01v"
   },
   "outputs": [],
   "source": [
    "!/usr/local/hadoop-3.3.1/bin/hdfs namenode -format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zhf-zK7NcBDF"
   },
   "outputs": [],
   "source": [
    "!/usr/local/hadoop-3.3.1/bin/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMh5I6cWSrDv"
   },
   "source": [
    "##<a name=\"end\"></a>The Word Count Problem \n",
    "\n",
    "The word count application counts the number of occurrences of all words appearing in a set of documents. As discussed in the previous lectures, the word count problem is a classic example of a MapReduce task. In this tutorial, we will run a simple MapReduce implementation of a word count application. This is going to serve as the [Hello World](https://en.wikipedia.org/wiki/%22Hello,_World!%22_program) of Hadoop MapReduce!  \n",
    "\n",
    "### Getting the Code \n",
    "\n",
    "You can find a simple Python implementation from the Block 9 git repo. To obtain the code clone the repo to Google Colab  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT9C-5zVT5aq"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/apanqasem/hadoop_map_reduce.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jGVp6TuUHVS"
   },
   "source": [
    "The mapper and reducer codes can be found in the `Hadoop/mapper.py` and `Hadoop/reducer.py` files. For convenience, let's copy them to the current directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNbrL7Lmgb8b"
   },
   "outputs": [],
   "source": [
    "!cp hadoop_map_reduce/mapper.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7WDIwQ5gm1i"
   },
   "outputs": [],
   "source": [
    "!cp hadoop_map_reduce/reducer.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyvRcSuFV0Bm"
   },
   "source": [
    "To be able to execute these codes, we will need to set the execute permission on the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RggUUHNfgsIR"
   },
   "outputs": [],
   "source": [
    "!chmod u+x mapper.py\n",
    "!chmod u+rwx reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZIPAfyAO5Ap"
   },
   "source": [
    "### Input and Output \n",
    "\n",
    "For any Hadoop job, we need to provide the names of an `input` and an `output` directory. The input directory is the place where the program is going to look for its input data. The output directory is the location where the output is going to be written. \n",
    "\n",
    "These directories can be given any names. By convention, the names typically contain input/output as a suffix or a prefix. Let's create the input directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uI-YBPIzcBCA"
   },
   "outputs": [],
   "source": [
    "!mkdir test_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2xVEFgWSA9X"
   },
   "source": [
    "The output directory will be automatically created when the Hadoop job runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6IlYraCSlmh"
   },
   "source": [
    "### Creating the input. \n",
    "\n",
    "Now, we will need to supply the input text files for our word count program to analyze. For this very simple example, we will just create a couple of files with a few words in them. This will allow us to visually verify that the program is runnning correctly. In the next tutorial, we will learn how to run Hadoop MapReduce with real datasets. \n",
    "\n",
    "We can create short text files using the Linux echo command. We will create five files containing lyrics from [Hamilton](https://en.wikipedia.org/wiki/Hamilton_(musical))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvBOgTvvdUUq"
   },
   "outputs": [],
   "source": [
    "!echo \"The ten-dollar founding father without a father\" > hamilton1.txt\n",
    "!echo \"Got a lot farther by working a lot harder\" > hamilton2.txt\n",
    "!echo \"By being a lot smarter\" > hamilton3.txt\n",
    "!echo \"By being a self-starter\" > hamilton4.txt\n",
    "!echo \"By fourteen, they placed him in charge of a trading charter\" > hamilton5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSnavduZVGRH"
   },
   "source": [
    "Now, we will move these files to the input directory, we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gCx4WFZe-aD"
   },
   "outputs": [],
   "source": [
    "!mv hamilton*.txt test_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lItkk4TLW24E"
   },
   "source": [
    "### Running the job\n",
    "\n",
    "Now we are all set to run our first MapReduce Hadoop job. \n",
    "\n",
    "The general format for the command for running a MapReduce Haddop job is as follows: \n",
    "\n",
    "```bash \n",
    "hadoop jar hadoop-streaming.jar \\\n",
    "-input name_of_input_file \\\n",
    "-output name_of_output_directory \\\n",
    "-file name_of_mapper_file \\\n",
    "-mapper the_mapper_cmd \\\n",
    "-file name_of_reducer_file \\\n",
    "-reducer the_reducer_cmd \\\n",
    "```\n",
    "\n",
    "That's a pretty long command! Let's break it down. \n",
    "\n",
    "  * `hadoop` says that we are launching a Hadoop task \n",
    "  * `jar` is the command to execute a Java program.\n",
    "  * `hadoop-streaming.jar` is a utility that comes with Haddop; it converts mapper and reducer code written in a different programming language to run in a MapReduce pattern in Hadoop; in this example we are feeding Python but we could also have written the coee in Ruby, R etc. \n",
    "  * `-input name_of_input_file` specifies the name of the input file for the MapReduce application \n",
    "  * `-output name_of_output_directory` is the output directory \n",
    "  * `-file name_of_mapper_file` is the name of the mapper file \n",
    "  * `-file name_of_reducer_file` is the name of the reducer file \n",
    "  * `-mapper the_mapper_cmd` is the actual command that needs to be executed to run the mapper \n",
    "  * `-reducer the_reducer_cmd` is the actual command that needs to be executed to run the reducer\n",
    "\n",
    "Our actual command for running the job is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYShAt24g-P-"
   },
   "outputs": [],
   "source": [
    "!rm -rf test_output\n",
    "!/usr/local/hadoop-3.3.1/bin/hadoop jar /usr/local/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar -input test_input/hamilton*.txt -output test_output -file mapper.py  -file reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wzt7xscjVDsI"
   },
   "source": [
    "If everything went ok, you should see a whole bunch of output. The last line should give you the name of the output directory. \n",
    "\n",
    "Let's check the contents of the output directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtr0xWbfcA5J"
   },
   "outputs": [],
   "source": [
    "!ls -ltr test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RMjutnYd0dF"
   },
   "source": [
    "`part-00000` is the file that should contain the results of output of the word count program. Let's check the contents of that file. You should see a recent timestamp on this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrAS6cQTfClZ"
   },
   "outputs": [],
   "source": [
    "!cat test_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCYWXrHfelrd"
   },
   "source": [
    "## <a name=\"end\"></a>Conclusion\n",
    "\n",
    "Does the output look correct? If so, then you have just run your first MapReduce job on a Hadoop cluster!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
